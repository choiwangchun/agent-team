# Optional: enable PostgreSQL persistence
DATABASE_URL=postgres://user:password@localhost:5432/kairo_agent

# Local file persistence (used only when DATABASE_URL is empty)
# Keeps provider OAuth/API auth state across local restarts.
MEMORY_REPO_PERSIST=true
MEMORY_REPO_STATE_FILE=.cache/memory-repository.json

# Server
PORT=3000
AUTH_DISABLED=true

# Upload / queue
MAX_UPLOAD_SIZE_MB=20
JOB_POLL_INTERVAL_MS=750
JOB_BATCH_SIZE=5
WORKFLOW_POLL_INTERVAL_MS=900
WORKFLOW_BATCH_SIZE=3
WORKFLOW_TASK_DELAY_MS=100

# Local autonomous execution (Node runtime only; disabled on serverless)
WORKFLOW_LOCAL_EXEC_ENABLED=false
WORKFLOW_LOCAL_EXEC_MAX_STEPS=3
WORKFLOW_LOCAL_EXEC_MAX_COMMANDS=3
WORKFLOW_LOCAL_EXEC_TIMEOUT_MS=45000
WORKFLOW_LOCAL_EXEC_OUTPUT_MAX_CHARS=12000
WORKFLOW_LOCAL_EXEC_CWD=/absolute/path/to/kairo-agent
WORKFLOW_LOCAL_EXEC_SHELL=/bin/zsh
WORKFLOW_NODE_SESSION_ROOT=.cache/workflow-node-sessions
WORKFLOW_NODE_MEMORY_TAIL_MAX=20
WORKFLOW_NODE_MEMORY_SUMMARY_MAX=4000
WORKFLOW_AGENT_TEAMS_ENABLED=true
WORKFLOW_AGENT_INBOX_MAX_MESSAGES=20
WORKFLOW_AGENT_OUTBOX_MAX_MESSAGES=8

# OpenClaw bridge for workflow task execution
WORKFLOW_OPENCLAW_BRIDGE_ENABLED=true
WORKFLOW_OPENCLAW_BIN=openclaw
WORKFLOW_OPENCLAW_DEFAULT_AGENT_ID=main
WORKFLOW_OPENCLAW_DEFAULT_PROVIDER=openai-codex
WORKFLOW_OPENCLAW_DEFAULT_MODEL=gpt-5.3-codex
WORKFLOW_OPENCLAW_USE_LOCAL=false
WORKFLOW_OPENCLAW_TIMEOUT_SEC=45
OPENCLAW_AUTH_SYNC_TTL_MS=15000
OPENCLAW_AUTH_SYNC_TIMEOUT_MS=15000
OPENCLAW_AUTH_SYNC_MAX_BYTES=2097152
OPENCLAW_AGENT_IMPORT_ENABLED=true
OPENCLAW_AGENT_IMPORT_TTL_MS=10000
# Optional override. If empty, uses:
# 1) ./.openclaw/openclaw.json
# 2) ~/.openclaw/openclaw.json
OPENCLAW_AGENT_CONFIG_PATH=

# Data AI processing
DATA_AI_MAX_ROWS=5000
DATA_AI_MAX_COLUMNS=120
DATA_AI_CELL_MAX_LENGTH=2048
DATA_AI_SAMPLE_ROWS=40
DATA_AI_MODEL_TIMEOUT_MS=45000
DATA_AI_RUN_TTL_SEC=1800
# Local Python tool for table transformation (serverless defaults to false)
DATA_AI_PYTHON_TOOL_ENABLED=true
DATA_AI_PYTHON_BIN=python3
DATA_AI_PYTHON_TIMEOUT_MS=20000
DATA_AI_PYTHON_SCRIPT=scripts/data_transformer.py

# Existing dashboard cache/metrics knobs
MICROCACHE_TTL_MS=0
LATENCY_SAMPLE_SIZE=2048

# Optional: Supabase Storage for avatar catalog
# If set, /api/avatars/random will prioritize Supabase bucket files.
SUPABASE_URL=
SUPABASE_SERVICE_ROLE_KEY=
SUPABASE_AVATAR_BUCKET=avatars
SUPABASE_AVATAR_PREFIX=
SUPABASE_AVATAR_PUBLIC=true
SUPABASE_AVATAR_SIGNED_URL_EXPIRES_SEC=3600
SUPABASE_AVATAR_MAX_FILES=5000
